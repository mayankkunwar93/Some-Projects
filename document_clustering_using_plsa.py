# -*- coding: utf-8 -*-
"""DOCUMENT_CLUSTERING_USING_PLSA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vetTPKRtvjCeNM4vDpTSrXOfiFO-CfZH
"""

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from pylab import random

"""**20 NewsGroups Dataset**"""

news = fetch_20newsgroups(subset='train', shuffle=True)
number_of_documents = 20       # taking few documents to cluster

observation_matrix = CountVectorizer(stop_words='english').fit_transform(news.data[:number_of_documents])
M = observation_matrix


labels = news.target
K = np.unique(labels).shape[0]

m = M.shape[0]    # No. of documents
n = M.shape[1]    # No. of words(excluding english language stop words)

print(K)

print(m)

print(n)

def EStep():
    for i in range(0, m):
        for j in range(0, n):
            denominator = 0;
            for k in range(0, K):
                p[i, j, k] = theta[k, j] * lamda[i, k];
                denominator += p[i, j, k];
            if denominator == 0:
                for k in range(0, K):
                    p[i, j, k] = 0;
            else:
                for k in range(0, K):
                    p[i, j, k] /= denominator;

def LogLikelihood():
    loglikelihood = 0
    for i in range(0, m):
        for j in range(0, n):
            tmp = 0
            for k in range(0, K):
                tmp += theta[k, j] * lamda[i, k]
            if tmp > 0:
                loglikelihood += M[i, j] * np.log(tmp)
    return loglikelihood

def initializeParameters():
    for i in range(0, m):
        normalization = sum(lamda[i, :])
        for j in range(0, K):
            lamda[i, j] /= normalization;

    for i in range(0, K):
        normalization = sum(theta[i, :])
        for j in range(0, n):
            theta[i, j] /= normalization;

# lamda[i, j] : p(tj|di)
lamda = random([m, K])

# theta[i, j] : p(wj|ti)
theta = random([K, n])

# p[i, j, k] : p(tk|di,wj)
p = np.zeros([m, n, K])
initializeParameters()

# EM algorithm
maxIteration = 3

for i in range(0, maxIteration):
    EStep()
    #Mstep
    # update theta # theta[i, j] = p(wj|ti)
    for k in range(0, K):
        denominator = 0
        for j in range(0, n):
            theta[k, j] = 0
            for i in range(0, m):
                theta[k, j] += M[i, j] * p[i, j, k]
            denominator += theta[k, j]
        if denominator == 0:
            for j in range(0, n):
                theta[k, j] = 1.0 / n
        else:
            for j in range(0, n):
                theta[k, j] /= denominator
        
    # update lamda # lamda[i, j] = p(tj|di)

    t_mat = np.zeros((m, K))
    for i in range(0, m):
      for k in range(0, K):
          lamda[i, k] = 0
          denominator = 0
          lamda[i, -1] = 0

          for j in range(0, n):
            lamda[i, k] += M[i, j] * p[i, j, k]
            denominator += M[i, j];
              
          if denominator == 0:
            lamda[i, k] = 1.0 / K 
          else:
            lamda[i, k] /= denominator  

          t_mat[i][k] = lamda[i, k]

print(t_mat)

"""**Now we will assign the LATENT TOPICS for each document**"""

t_star = []
for i in range(0, m):
  t_star.append(np.argmax(t_mat[i]))

print(t_star)

print(np.size(t_star))

#Cluster numbers and Document numbers are starting from 0
for k in range(0, K):
  doc = []
  for i in range(0, m):
    if t_star[i] == k:
      doc.append(i)
  if doc == []:
    print("Documents in cluster ",k,"     : NO DOCUMENTS FOUND")
  else:
    print("Documents in cluster ",k," are : ","Document Number ",doc)